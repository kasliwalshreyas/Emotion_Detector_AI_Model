{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Neural network using Convolutional Neural Networks and OpenCV to detect face emotions, with 7 target labels (angry,disgust,fear,happy,neutral,sad,surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of dataset\n",
    "\n",
    "loc = \"C:/Users/91808/Downloads/SPK IIIT/datasets/Emotion Detector Dataset/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Folder  angry\n",
      "Loaded Folder  disgust\n",
      "Loaded Folder  fear\n",
      "Loaded Folder  happy\n",
      "Loaded Folder  neutral\n",
      "Loaded Folder  sad\n",
      "Loaded Folder  surprise\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "target = []\n",
    "for i in [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"]:\n",
    "    \n",
    "    collection_images_name = os.listdir(loc + str(i))\n",
    "    for j in collection_images_name:\n",
    "        img = cv2.imread(loc + str(i) + \"/\" + j)\n",
    "        try:\n",
    "            img = cv2.resize(img,(32,32))\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            features.append(img)\n",
    "            if i == \"angry\":\n",
    "                target.append(0)\n",
    "            elif i == \"disgust\":\n",
    "                target.append(1)\n",
    "            elif i == \"fear\":\n",
    "                target.append(2)\n",
    "            elif i == \"happy\":\n",
    "                target.append(3)\n",
    "            elif i == \"neutral\":\n",
    "                target.append(4)\n",
    "            elif i == \"sad\":\n",
    "                target.append(5)\n",
    "            elif i == \"surprise\":\n",
    "                target.append(6)\n",
    "    print(\"Loaded Folder \",i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location of dataset\n",
    "\n",
    "address = \"C:/Users/91808/Downloads/SPK IIIT/datasets/Emotion Detector Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "dataSet_train = ImageDataGenerator()\n",
    "dataSet_val = ImageDataGenerator()\n",
    "\n",
    "train_features = dataSet_train.flow_from_directory(address + \"train\",\n",
    "                                                   target_size=(48,48),\n",
    "                                                   batch_size= 128,\n",
    "                                                   color_mode=\"grayscale\",\n",
    "                                                   class_mode = \"categorical\",\n",
    "                                                   )\n",
    "\n",
    "test_features = dataSet_val.flow_from_directory(address + \"validation\",\n",
    "                                                   target_size=(48,48),\n",
    "                                                   batch_size= 128,\n",
    "                                                   color_mode=\"grayscale\",\n",
    "                                                   class_mode = \"categorical\",\n",
    "                                                \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,MaxPooling2D,Conv2D,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",input_shape = (48,48,1)))\n",
    "model.add(Conv2D(64,(3,3),activation = \"relu\"))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(128,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(7,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(Adam(learning_rate=0.001),loss = \"categorical_crossentropy\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nidhish\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "225/225 [==============================] - 168s 747ms/step - loss: 2.6726 - accuracy: 0.2469 - val_loss: 1.7562 - val_accuracy: 0.3048\n",
      "Epoch 2/50\n",
      "225/225 [==============================] - 123s 548ms/step - loss: 1.7427 - accuracy: 0.2910 - val_loss: 1.6939 - val_accuracy: 0.3328\n",
      "Epoch 3/50\n",
      "225/225 [==============================] - 123s 545ms/step - loss: 1.7021 - accuracy: 0.3127 - val_loss: 1.6408 - val_accuracy: 0.3602\n",
      "Epoch 4/50\n",
      "225/225 [==============================] - 136s 606ms/step - loss: 1.6600 - accuracy: 0.3347 - val_loss: 1.6074 - val_accuracy: 0.3778\n",
      "Epoch 5/50\n",
      "225/225 [==============================] - 136s 606ms/step - loss: 1.6103 - accuracy: 0.3601 - val_loss: 1.5786 - val_accuracy: 0.3814\n",
      "Epoch 6/50\n",
      "225/225 [==============================] - 136s 602ms/step - loss: 1.5221 - accuracy: 0.4053 - val_loss: 1.4626 - val_accuracy: 0.4347\n",
      "Epoch 7/50\n",
      "225/225 [==============================] - 137s 607ms/step - loss: 1.4572 - accuracy: 0.4291 - val_loss: 1.3805 - val_accuracy: 0.4747\n",
      "Epoch 8/50\n",
      "225/225 [==============================] - 136s 604ms/step - loss: 1.3911 - accuracy: 0.4600 - val_loss: 1.3261 - val_accuracy: 0.5043\n",
      "Epoch 9/50\n",
      "225/225 [==============================] - 136s 605ms/step - loss: 1.3446 - accuracy: 0.4819 - val_loss: 1.3114 - val_accuracy: 0.5176\n",
      "Epoch 10/50\n",
      "225/225 [==============================] - 136s 604ms/step - loss: 1.2991 - accuracy: 0.5038 - val_loss: 1.2756 - val_accuracy: 0.5226\n",
      "Epoch 11/50\n",
      "225/225 [==============================] - 136s 604ms/step - loss: 1.2589 - accuracy: 0.5199 - val_loss: 1.2243 - val_accuracy: 0.5389\n",
      "Epoch 12/50\n",
      "225/225 [==============================] - 130s 577ms/step - loss: 1.2113 - accuracy: 0.5371 - val_loss: 1.2201 - val_accuracy: 0.5432\n",
      "Epoch 13/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 1.1785 - accuracy: 0.5520 - val_loss: 1.1718 - val_accuracy: 0.5615\n",
      "Epoch 14/50\n",
      "225/225 [==============================] - 122s 544ms/step - loss: 1.1372 - accuracy: 0.5670 - val_loss: 1.1772 - val_accuracy: 0.5605\n",
      "Epoch 15/50\n",
      "225/225 [==============================] - 121s 540ms/step - loss: 1.1038 - accuracy: 0.5777 - val_loss: 1.1625 - val_accuracy: 0.5638\n",
      "Epoch 16/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 1.0657 - accuracy: 0.5986 - val_loss: 1.1491 - val_accuracy: 0.5716\n",
      "Epoch 17/50\n",
      "225/225 [==============================] - 122s 540ms/step - loss: 1.0360 - accuracy: 0.6112 - val_loss: 1.1353 - val_accuracy: 0.5739\n",
      "Epoch 18/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.9974 - accuracy: 0.6278 - val_loss: 1.1335 - val_accuracy: 0.5803\n",
      "Epoch 19/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.9580 - accuracy: 0.6441 - val_loss: 1.1370 - val_accuracy: 0.5778\n",
      "Epoch 20/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.9174 - accuracy: 0.6536 - val_loss: 1.1425 - val_accuracy: 0.5749\n",
      "Epoch 21/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.8855 - accuracy: 0.6705 - val_loss: 1.1408 - val_accuracy: 0.5832\n",
      "Epoch 22/50\n",
      "225/225 [==============================] - 121s 538ms/step - loss: 0.8596 - accuracy: 0.6819 - val_loss: 1.1207 - val_accuracy: 0.5827\n",
      "Epoch 23/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.8142 - accuracy: 0.6982 - val_loss: 1.1262 - val_accuracy: 0.5893\n",
      "Epoch 24/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.7878 - accuracy: 0.7086 - val_loss: 1.1530 - val_accuracy: 0.5905\n",
      "Epoch 25/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.7592 - accuracy: 0.7197 - val_loss: 1.1573 - val_accuracy: 0.5903\n",
      "Epoch 26/50\n",
      "225/225 [==============================] - 122s 543ms/step - loss: 0.7147 - accuracy: 0.7375 - val_loss: 1.1542 - val_accuracy: 0.5874\n",
      "Epoch 27/50\n",
      "225/225 [==============================] - 122s 543ms/step - loss: 0.6803 - accuracy: 0.7471 - val_loss: 1.1725 - val_accuracy: 0.5896\n",
      "Epoch 28/50\n",
      "225/225 [==============================] - 126s 561ms/step - loss: 0.6619 - accuracy: 0.7586 - val_loss: 1.1748 - val_accuracy: 0.5977\n",
      "Epoch 29/50\n",
      "225/225 [==============================] - 132s 586ms/step - loss: 0.6288 - accuracy: 0.7693 - val_loss: 1.1891 - val_accuracy: 0.5862\n",
      "Epoch 30/50\n",
      "225/225 [==============================] - 129s 572ms/step - loss: 0.5914 - accuracy: 0.7858 - val_loss: 1.2087 - val_accuracy: 0.5895\n",
      "Epoch 31/50\n",
      "225/225 [==============================] - 122s 540ms/step - loss: 0.5864 - accuracy: 0.7878 - val_loss: 1.2020 - val_accuracy: 0.5970\n",
      "Epoch 32/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.5688 - accuracy: 0.7959 - val_loss: 1.2053 - val_accuracy: 0.5939\n",
      "Epoch 33/50\n",
      "225/225 [==============================] - 123s 547ms/step - loss: 0.5307 - accuracy: 0.8084 - val_loss: 1.2182 - val_accuracy: 0.5946\n",
      "Epoch 34/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.5217 - accuracy: 0.8137 - val_loss: 1.2500 - val_accuracy: 0.5973\n",
      "Epoch 35/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.4972 - accuracy: 0.8228 - val_loss: 1.2897 - val_accuracy: 0.5994\n",
      "Epoch 36/50\n",
      "225/225 [==============================] - 122s 542ms/step - loss: 0.4809 - accuracy: 0.8288 - val_loss: 1.3023 - val_accuracy: 0.5864\n",
      "Epoch 37/50\n",
      "225/225 [==============================] - 122s 542ms/step - loss: 0.4725 - accuracy: 0.8328 - val_loss: 1.3046 - val_accuracy: 0.5947\n",
      "Epoch 38/50\n",
      "225/225 [==============================] - 121s 539ms/step - loss: 0.4490 - accuracy: 0.8421 - val_loss: 1.2834 - val_accuracy: 0.5986\n",
      "Epoch 39/50\n",
      "225/225 [==============================] - 124s 550ms/step - loss: 0.4317 - accuracy: 0.8472 - val_loss: 1.2855 - val_accuracy: 0.6009\n",
      "Epoch 40/50\n",
      "225/225 [==============================] - 122s 543ms/step - loss: 0.4248 - accuracy: 0.8513 - val_loss: 1.3024 - val_accuracy: 0.5893\n",
      "Epoch 41/50\n",
      "225/225 [==============================] - 121s 540ms/step - loss: 0.4191 - accuracy: 0.8508 - val_loss: 1.3622 - val_accuracy: 0.5957\n",
      "Epoch 42/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.3982 - accuracy: 0.8587 - val_loss: 1.3099 - val_accuracy: 0.6041\n",
      "Epoch 43/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3995 - accuracy: 0.8587 - val_loss: 1.3614 - val_accuracy: 0.5946\n",
      "Epoch 44/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3818 - accuracy: 0.8653 - val_loss: 1.3656 - val_accuracy: 0.5918\n",
      "Epoch 45/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3782 - accuracy: 0.8687 - val_loss: 1.4072 - val_accuracy: 0.5947\n",
      "Epoch 46/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3499 - accuracy: 0.8784 - val_loss: 1.3620 - val_accuracy: 0.5986\n",
      "Epoch 47/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3644 - accuracy: 0.8743 - val_loss: 1.3668 - val_accuracy: 0.5915\n",
      "Epoch 48/50\n",
      "225/225 [==============================] - 122s 541ms/step - loss: 0.3619 - accuracy: 0.8759 - val_loss: 1.4016 - val_accuracy: 0.5928\n",
      "Epoch 49/50\n",
      "225/225 [==============================] - 121s 537ms/step - loss: 0.3459 - accuracy: 0.8807 - val_loss: 1.4636 - val_accuracy: 0.5854\n",
      "Epoch 50/50\n",
      "225/225 [==============================] - 120s 535ms/step - loss: 0.3421 - accuracy: 0.8840 - val_loss: 1.4063 - val_accuracy: 0.5947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c2033b5460>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_features,\n",
    "                    steps_per_epoch= train_features.n // train_features.batch_size,\n",
    "                    epochs= 50,\n",
    "                    validation_data= test_features,\n",
    "                    validation_steps= test_features.n // test_features.batch_size \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of folder where, files of model are to saved\n",
    "\n",
    "location = \"C:/Users/91808/Downloads/SPK IIIT/internships and courses/python/emotion detector model files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelInJson = model.to_json()\n",
    "abc = open(location + \"model_emotion_detector.json\",\"w\")\n",
    "abc.write(ModelInJson)\n",
    "\n",
    "model.save_weights(location + \"model_emotion_detector_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of folder from where, files of model are to loaded\n",
    "\n",
    "location = \"C:/Users/91808/Downloads/SPK IIIT/internships and courses/python/emotion detector model files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "abc = open(location + \"model_emotion_detector.json\",\"r\")\n",
    "loaded_data = abc.read()\n",
    "loaded_model = model_from_json(loaded_data)\n",
    "loaded_model.load_weights(location + \"model_emotion_detector_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location of haarcascade_frontalface_default file\n",
    "\n",
    "place = \"C:/Users/91808/Downloads/SPK IIIT/internships and courses/python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotions = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640)\n",
    "cap.set(4,480)\n",
    "cap.set(10,180)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    facecasc = cv2.CascadeClassifier(place + \"haarcascade_frontalface_default.xml\")\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = loaded_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame,\"Emotion: \",(20,35),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,225),2)\n",
    "        cv2.putText(frame,\"Probablity: \",(20,75),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,225),2)\n",
    "        ProbabilityValue = np.amax(prediction)\n",
    "        if ProbabilityValue> 0.25:\n",
    "            cv2.putText(frame,emotions[maxindex],(225,35),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,225),2)\n",
    "            cv2.putText(frame,str(int(ProbabilityValue*100)) + \"%\",(225,75),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,225),2)\n",
    "    cv2.imshow('Emotion Detector', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fd1b9c6c00a60c6076f8a1d23e68b0de7c6a29679a6d79b8bd8a58011edf3b8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
